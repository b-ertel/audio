{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZRpANpGcMQ4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ntx9mJfBw3yO"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchvision import  datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL6IIpoXcR5F"
   },
   "source": [
    "# Unpack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln6Cx9KRxB7T"
   },
   "outputs": [],
   "source": [
    "#!unzip upload_colab.zip\n",
    "!unzip upload_colab_stacked.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnUbBQ6pyK04",
    "outputId": "c4e0bb4a-43f1-4902-a8c7-0bb61d627bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAi1CZNVcIPt"
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a3DQJfh460iO"
   },
   "outputs": [],
   "source": [
    "class AudioImagesDataset(Dataset):\n",
    "    \"\"\"audio images dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_images_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_images_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.audio_images_frame.iloc[idx, 1])\n",
    "        image = io.imread(img_name)\n",
    "        audio_class = self.audio_images_frame.iloc[idx, 2]\n",
    "        file_name = self.audio_images_frame.iloc[idx, 1]\n",
    "        audio_class = audio_class.astype('int')\n",
    "        sample = {'image': image, 'audio_class': audio_class, 'file_name': file_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2nGbu-DcV7R"
   },
   "source": [
    "# Data Transformation Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nmc6KFOY-bzl"
   },
   "outputs": [],
   "source": [
    "#Transformation classes for the dataset\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, audio_class = sample['image'], sample['audio_class']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'audio_class': audio_class, 'file_name': sample['file_name']}\n",
    "\n",
    "\n",
    "class LeftCrop(object):\n",
    "    \"\"\"Crop the image from top left to given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, audio_class = sample['image'], sample['audio_class']\n",
    "\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        img = image[0: new_h,\n",
    "                      0: new_w]\n",
    "\n",
    "        return {'image': img, 'audio_class': audio_class, 'file_name': sample['file_name']}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, audio_class = sample['image'], sample['audio_class']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        image = image.transpose((1,0))\n",
    "        image = torch.from_numpy(image).to(torch.float)\n",
    "        image = image.unsqueeze(0)\n",
    "        return {'image': image,\n",
    "                'audio_class': audio_class, 'file_name': sample['file_name']}\n",
    "\n",
    "\n",
    "class ToNormalize(object):\n",
    "    \"\"\"Normalization of the image\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, audio_class = sample['image'], sample['audio_class']\n",
    "        \n",
    "        image = transforms.Normalize([133.09], [55.32]).forward(image)\n",
    "        \n",
    "        return {'image': image,\n",
    "                'audio_class': audio_class, 'file_name': sample['file_name']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KriIjJu-oyQC"
   },
   "source": [
    "## Calculating mean and standard deviation for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4eU6STUorjV",
    "outputId": "36095e27-123c-4923-a73c-04a619f30cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already calculated, see below\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "  print(\"Already calculated, see below\")\n",
    "else:\n",
    "  liste = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "  data_csv = 'audio_images/fold' +str(liste[0]) +'/filename_class.csv'\n",
    "  data_root = 'audio_images/fold' +str(liste[0]) +'/'\n",
    "  dataset = AudioImagesDataset(csv_file = data_csv, root_dir = data_root, transform=audio_transform)\n",
    "  for fold in liste[1:]:\n",
    "    data_csv = 'audio_images/fold' +str(fold) +'/filename_class.csv'\n",
    "    data_root = 'audio_images/fold' +str(fold) +'/'\n",
    "    next_fold = AudioImagesDataset(csv_file = data_csv, root_dir = data_root, transform = audio_transform )\n",
    "    dataset = torch.utils.data.ConcatDataset([dataset, next_fold])\n",
    "  loader = DataLoader(dataset, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "  mean = 0.0\n",
    "  for i, data in enumerate(loader, 0):\n",
    "      images = data['image'].to(device)\n",
    "      batch_samples = images.size(0) \n",
    "      images = images.view(batch_samples, images.size(1), -1)\n",
    "      mean += images.mean(2).sum(0)\n",
    "  mean = mean / len(loader.dataset)\n",
    "\n",
    "  var = 0.0\n",
    "  for i, data in enumerate(loader, 0):\n",
    "      images = data['image'].to(device)\n",
    "      batch_samples = images.size(0)\n",
    "      images = images.view(batch_samples, images.size(1), -1)\n",
    "      var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "  std = torch.sqrt(var / (len(loader.dataset)*128*128))\n",
    "\n",
    "  print(mean)\n",
    "  print(std)\n",
    "  del(dataset)\n",
    "  del(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyvu_DB_y9tE"
   },
   "source": [
    "tensor([133.0939], device='cuda:0')\n",
    "\n",
    "tensor([55.3163], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "y8vOO0SRVpTK"
   },
   "outputs": [],
   "source": [
    "audio_transform = transforms.Compose([\n",
    "                                               LeftCrop(127), \n",
    "                                               ToTensor(),\n",
    "                                               ToNormalize()\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhhqhYuicbO7"
   },
   "source": [
    "# The network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "b0FUhRlUdq0r"
   },
   "outputs": [],
   "source": [
    "class vgg_ten_label(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg_ten_label, self).__init__()\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        \n",
    "        features = list(model.features.children())\n",
    "        features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.features = nn.Sequential(*features)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.classifier = nn.Sequential(*model.classifier.children())\n",
    "        self.classifier_extra = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000 , 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512 , 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.classifier_extra(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL0rS-9MT0lj",
    "outputId": "946372b8-c145-4cb7-a6b7-76bf1c435ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_ten_label(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (classifier_extra): Sequential(\n",
      "    (0): ReLU(inplace=True)\n",
      "    (1): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg_ten_label().to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fePfGdDic47S"
   },
   "source": [
    "# Helpful functions for Visualization and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "s9Yq9jHv4iLH"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "2Y8kKjaQnxOH"
   },
   "outputs": [],
   "source": [
    "def calculate_print_accuracy(model, test_loader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  true_classes = np.array([])\n",
    "  predicted_classes = np.array([])\n",
    "  with torch.no_grad():\n",
    "      for i, data in enumerate(test_loader,0):\n",
    "          images, labels = data['image'].to(device), data['audio_class'].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "          true_classes = np.append(true_classes, labels.cpu())\n",
    "          predicted_classes = np.append(predicted_classes, predicted.cpu())\n",
    "\n",
    "  print('Accuracy of the network on the test images: %d %%' % (\n",
    "      100 * correct / total))\n",
    "  accuracy = correct / total\n",
    "  conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "  return conf_matrix, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jra6d5MVdAxC"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "9IhrcH5NzQFj"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader):\n",
    "  for epoch in range(18):  # loop over the dataset multiple times\n",
    "\n",
    "      running_loss = 0.0\n",
    "      for i, data in enumerate(train_loader, 0):\n",
    "          # get the inputs; data is a dict\n",
    "          inputs, labels = data['image'].to(device), data['audio_class'].to(device)\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "          # forward + backward + optimize\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          # add Loss for statistic in print-function\n",
    "          running_loss += loss.item()\n",
    "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss ))\n",
    "      running_loss = 0.0\n",
    "\n",
    "  print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1QOXuCHdDsU"
   },
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQ5yobE6jPzY",
    "outputId": "cc030d98-713e-4b8f-88d0-a5e0a70a281e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on fold #1\n",
      "Before training the network:\n"
     ]
    }
   ],
   "source": [
    "#########Using cross validation ###############\n",
    "accuracies = []\n",
    "confusion_matrices = []\n",
    "for cross_idx in range(1,11):\n",
    "  print(\"Validation on fold #\" + str(cross_idx))\n",
    "  #prep for datasets\n",
    "  liste = [1,2,3,4,5,6,7,8,9,10]\n",
    "  liste.remove(cross_idx)\n",
    "\n",
    "  #testset \n",
    "  test_csv = 'audio_images/fold' +str(cross_idx) +'/filename_class.csv'\n",
    "  test_root = 'audio_images/fold' +str(cross_idx) +'/'\n",
    "  testset = AudioImagesDataset(csv_file = test_csv, root_dir = test_root, transform=audio_transform)\n",
    "  testloader = DataLoader(testset, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)\n",
    "  \n",
    "  #trainset\n",
    "  train_csv = 'audio_images/fold' +str(liste[0]) +'/filename_class.csv'\n",
    "  train_root = 'audio_images/fold' +str(liste[0]) +'/'\n",
    "  trainset = AudioImagesDataset(csv_file = train_csv, root_dir = train_root, transform=audio_transform)\n",
    "  for fold in liste[1:]:\n",
    "    train_csv = 'audio_images/fold' +str(fold) +'/filename_class.csv'\n",
    "    train_root = 'audio_images/fold' +str(fold) +'/'\n",
    "    next_fold = AudioImagesDataset(csv_file = train_csv, root_dir = train_root, transform = audio_transform )\n",
    "    trainset = torch.utils.data.ConcatDataset([trainset, next_fold])\n",
    "  trainloader = DataLoader(trainset, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)\n",
    "  \n",
    "\n",
    "  #initialize model\n",
    "  vgg_ten = vgg_ten_label().to(device)\n",
    "  print(\"Before training the network:\")\n",
    "  conf, acc = calculate_print_accuracy(vgg_ten, testloader)\n",
    "  #plot_confusion_matrix(conf, [\"AC\", \"CH\", \"CP\", \"DB\", \"DR\", \"EI\", \"GS\", \"JH\", \"SI\", \"SM\"])\n",
    "  \n",
    "  # Define the loss and the optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer_vgg_ten= torch.optim.SGD(vgg_ten.parameters(), lr=0.001, momentum=0.9)\n",
    "  train_model(vgg_ten, criterion, optimizer_vgg_ten, trainloader)\n",
    "  print(\"After training the network:\")\n",
    "  conf, acc = calculate_print_accuracy(vgg_ten, testloader)\n",
    "  plot_confusion_matrix(conf, [\"AC\", \"CH\", \"CP\", \"DB\", \"DR\", \"EI\", \"GS\", \"JH\", \"SI\", \"SM\"])\n",
    "  accuracies.append(acc)\n",
    "  confusion_matrices.append(conf)\n",
    "  print(\"================================================================================\")\n",
    "  print(\"================================================================================\")\n",
    "  print(\"================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaiWG5i4kiI3"
   },
   "outputs": [],
   "source": [
    "np.average(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1BmajXXjkaX"
   },
   "outputs": [],
   "source": [
    "combined = confusion_matrices[0] + confusion_matrices[1]\n",
    "for i in range(2,10):\n",
    "  combined = combined + confusion_matrices[i]\n",
    "\n",
    "plot_confusion_matrix(combined, [\"AC\", \"CH\", \"CP\", \"DB\", \"DR\", \"EI\", \"GS\", \"JH\", \"SI\", \"SM\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Audio_classification_deaf_machine.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
